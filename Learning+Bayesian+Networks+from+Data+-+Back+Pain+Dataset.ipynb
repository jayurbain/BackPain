{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Learning Bayesian Networks\n",
    "\n",
    "\n",
    "Previous notebooks showed how Bayesian networks economically encode a probability distribution over a set of variables, and how they can be used e.g. to predict variable states, or to generate new samples from the joint distribution. This section will be about obtaining a Bayesian network, given a set of sample data. Learning a Bayesian network can be split into two problems:\n",
    "\n",
    " **Parameter learning:** Given a set of data samples and a DAG that captures the dependencies between the variables, estimate the (conditional) probability distributions of the individual variables.\n",
    " \n",
    " **Structure learning:** Given a set of data samples, estimate a DAG that captures the dependencies between the variables.\n",
    " \n",
    "This notebook aims to illustrate how parameter learning and structure learning can be done with pgmpy.\n",
    "Currently, the library supports:\n",
    " - Parameter learning for *discrete* nodes:\n",
    "   - Maximum Likelihood Estimation\n",
    "   - Bayesian Estimation\n",
    " - Structure learning for *discrete*, *fully observed* networks:\n",
    "   - Score-based structure estimation (BIC/BDeu/K2 score; exhaustive search, hill climb/tabu search)\n",
    "   - Constraint-based structure estimation (PC)\n",
    "   - Hybrid structure estimation (MMHC)\n",
    "\n",
    "\n",
    "## Parameter Learning\n",
    "\n",
    "Suppose we have the following data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back Pain Data Analysis\n",
    "\n",
    "Jay Urbain, PhD\n",
    "Meredith Adams, MD\n",
    "\n",
    "10/25/2016, 11/5/2016, 11/21/2016, 11/23/2016\n",
    "\n",
    "Integration of statistical analysis and machine learning methods to identify factors associated with opioid prescribing in NIH research standards for Low Back Pain survey data: a pilot analysis.\n",
    "\n",
    "<a href=\"cLBP_RTF_MinimalDataset.pdf\">Standford Back Pain Survey</a>.\n",
    "\n",
    "<a href=\"data/stanford low back pain survey data.csv\">Data set</a> (csv).\n",
    "\n",
    "#### Introduction:\n",
    "\n",
    "The recent development of research standards for low back pain (NIH LBP taskforce reference) creates an opportunity for prospective data standardization. Ultimately, the goal of this standardized data collection is to better understand patterns for treatment response and build predictive care models. While the impact of aggregate data will depend on large-scale integration, the focus of this study is to better understand the relationship and predictive ability of the survey variables, specifically examining predictors of opioid use. \n",
    "Clinical research is evolving to reflect the need for efficient clinical trial design and data collection, which is reflected by the move toward improved data standardization. A key component of this is adaptive statistical designs and analysis methods. The expert consensus panel developed the NIH task force for research standards questionnaire (LBPTF) to overcome common research barriers while addressing the underlying key clinical questions for low back pain. Specifically, it shifted the focus from anatomic or pathophysiological classification to that of pain interference, functional status, and pain intensity. This focused questionnaire measures these domains using several short forms from PROMIS (Patient-Reported Outcome Measurement Information System).\n",
    "\n",
    "The novel organizational framework of the LBPTF questionnaire incorporates key clinical self-report measures as well as information about co-morbid conditions, demographic information, and treatment history. Understanding the co-occurence patterns of these data may provide insight into more focused data collection as well as build toward predictive modeling. The inherent limitations of self-reported data are mitigated by the extensive development of the minimum data set variables to incorporate key perceived domains of influence.\n",
    "\n",
    "Building from this perspective, the objective of this pilot survey was to deconstruct and analyze the inter-relationship of these variables in a way that will provide more meaningful analysis of these data moving forward. Statistical analysis and interpretation can be misleading due to inherent data assumptions, but with the data points selected by expert consensus, this minimum dataset represents the starting point for analyzing these relationships. Recognizing the limitations of a survey snapshot, we planned iterative analyses of a pilot survey obtained during the LBPTF with several statistical and machine learning methods to validate our approach.\n",
    "\n",
    "#### Results Summary\n",
    "\n",
    "We performed a preliminary statistical analysis of the back pain dateset.\n",
    "\n",
    "For the 200-sample dataset, Opioid use negatively affects the population distribution for physical function, physical interference, and pain intensity. All of these results are statistically significant (*p* << 0.05*) using Pearson's $\\chi^2$ (Chi-squared) goodness of fit statistic.\n",
    "\n",
    "We evaluate Logistic Regression (LR), Support Vector Machines (SVM), Decision Trees (DT), and recursive feature elimination for identifying the most predictive features in the data set.\n",
    "\n",
    "Using all features and 10-fold cross-validation, our predictive accuracies are *0.69, 0.63, and 0.58* for LR, SVM, and DT respectively. \n",
    "\n",
    "Using the top 10 features identified using recursive feature elimination for each classifier and 10-fold cross-validation, our predictive accuracies improvei modestly to *0.71, 0.66, and 0.66* for LR, SVM, and DT respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Legend\n",
    "\n",
    "Match columns in the data set to questions in the survey.\n",
    "\n",
    "\n",
    "DUR - 1. How long has low-back pain been an ongoing problem for you?  \n",
    "FREQ - 2. How often has low-back pain been an ongoing problem for you over the past 6 months?    \n",
    "NRS - 3. In the past 7 days, how would you rate your low-back pain on average?  \n",
    "RAD - 4. Has back pain spread down your leg(s) during the past 2 weeks? (radiculopathy)  \n",
    "PIDAY - 9. How much did pain interfere with your day-to-day activities?  \n",
    "PIWORK - 10. How much did pain interfere with work around the home?  \n",
    "PPISOC - 11. How much did pain interfere with your ability to participate in social activities?  \n",
    "PICHOR - 12. How much did pain interfere with your household chores?  \n",
    "LBS - 6. Have you ever had a low-back operation?   \n",
    "LBST - 7. If yes, when was your last back operation?  \n",
    "FUS - 8. Did any of your back operations involve a spinal fusion?    \n",
    "OPI - 13. Opioid painkillers, have you used for your back pain?  \n",
    "INJ - 13. Injections such as epidural steroid injections, facet injections, have you used for your back pain?  \n",
    "EXE - 13. Exercise therapy, have you used for your back pain?  \n",
    "PSY - 13. Psychological counseling, have you used for your back pain?  \n",
    "UNEMP - 14. I have been off work or unemployed for 1 month or more due to low-back pain.   \n",
    "DIS - 15. I receive or have applied for disability or workers’ compensation benefits because I am unable to work due to low-back pain.  \n",
    "ABD - 5. Stomach pain  \n",
    "JOI - 5. Pain in your arms, legs, or joints other than your spine or back  \n",
    "HEA - 5. Headaches # Widespread pain or pain in most of your body??  \n",
    "FIB - 15. I receive or have applied for disability benefits because I am unable to work dueto low-back pain.  \n",
    "CHOR - 16. Are you able to do chores such as vacuuming or yard work?  \n",
    "STAIR - 17. Are you able to go up and down stairs at a normal pace?  \n",
    "W15 - 18. Are you able to go for a walk of at least 15 minutes?  \n",
    "ERANDS 19. Are you able to run errands and shop?  \n",
    "WORTH -, 20. In the past 7 days, I felt worthless.  \n",
    "HELPL,  21. In the past 7 days, I felt helpless.  \n",
    "DEPRES -  22. In the past 7 days, I felt depressed.  \n",
    "HOPEL -  23.  In the past 7 days, I felt hopeless.  \n",
    "SLEEPQ - 24.  In the past 7 days, my sleep quality was (choices)  \n",
    "SREFR - 25.  In the past 7 days, my sleep was refreshing.  \n",
    "SPROB - 26.  In the past 7 days, I had a problem with my sleep.  \n",
    "SONSET - 27. I had difficulty falling asleep  \n",
    "CAT.SAFE - 28.  It’s not really safe for a person with my back problem to be physically active.  \n",
    "CAT.NEVER, 29.  I feel that my back pain is terrible and it’s never going to get any better.  \n",
    "LIT,  30.  Are you involved in a lawsuit or legal claim related to your back problem?  \n",
    "AS -  \n",
    "ETOH  - 31. Have you drunk or used drugs more than you meant to?\n",
    "SAHELP – 32. Have you felt you wanted or needed to cut down on your drinking or drug use?   \n",
    "AGE - 33. Age: years (0–120)   \n",
    "SEX - 34. Gender (Male/Female/Unknown/Unspecified)\n",
    "HIS – 35. Hispanic or Latino/Not H or L/Unknown/Unreported)  \n",
    "NAT - Native American\n",
    "ASA - Asian\n",
    "BL - Black or African American\n",
    "PAC - Native Hawaiian or Pacific Islander\n",
    "W -  White\n",
    "UNK - Unknown\n",
    "NA. – Not reported  \n",
    "EMP - 37. Employment Status  \n",
    "EDU - 38. Education Level: (select the highest level attained)   \n",
    "SMOK - 39. How would you describe your cigarette smoking?   \n",
    "HT - 40. Height  \n",
    "WT - 40. Weight  \n",
    "RACE - 36.  \n",
    "PI - ??  \n",
    "FUN - ??  \n",
    "DEP - 22. ??  \n",
    "SLEEP - 24. ??  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 60)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "%matplotlib inline \n",
    "\n",
    "# Read dataset into a Pandas dataframe\n",
    "df = pd.read_csv(\"data/stanford low back pain survey data.csv\")\n",
    "# Dimensions\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reveiw and clean the  data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               ID         DUR       FREQ         NRS         RAD       PIDAY  \\\n",
      "count  200.000000  200.000000  200.00000  199.000000  199.000000  200.000000   \n",
      "mean   115.520000    3.530000    2.45000    5.492462    0.422111    3.330000   \n",
      "std     68.958436    0.686627    0.72118    2.117434    0.495142    1.061477   \n",
      "min      5.000000    1.000000    1.00000    1.000000    0.000000    1.000000   \n",
      "25%     57.750000    3.000000    2.00000    4.000000    0.000000    3.000000   \n",
      "50%    107.500000    4.000000    3.00000    6.000000    0.000000    3.000000   \n",
      "75%    172.250000    4.000000    3.00000    7.000000    1.000000    4.000000   \n",
      "max    242.000000    4.000000    3.00000   10.000000    1.000000    5.000000   \n",
      "\n",
      "           PIWORK      PPISOC      PICHOR         LBS     ...             EMP  \\\n",
      "count  200.000000  199.000000  199.000000  200.000000     ...      200.000000   \n",
      "mean     3.430000    3.160804    3.407035    0.200000     ...        2.680000   \n",
      "std      1.091355    1.236757    1.154766    0.530478     ...        2.087917   \n",
      "min      1.000000    1.000000    1.000000    0.000000     ...        0.000000   \n",
      "25%      3.000000    2.000000    2.500000    0.000000     ...        1.000000   \n",
      "50%      3.000000    3.000000    4.000000    0.000000     ...        2.000000   \n",
      "75%      4.000000    4.000000    4.000000    0.000000     ...        5.000000   \n",
      "max      5.000000    5.000000    5.000000    2.000000     ...        9.000000   \n",
      "\n",
      "              EDU   SMOK          HT          WT        RACE          PI  \\\n",
      "count  200.000000  119.0  187.000000  188.000000  181.000000  198.000000   \n",
      "mean     4.905000    0.0   66.861497  174.928191    4.276243   13.308081   \n",
      "std      2.106695    0.0    4.411448   41.924957    1.312732    4.166708   \n",
      "min      0.000000    0.0   50.000000   96.000000    0.000000    4.000000   \n",
      "25%      3.000000    0.0   64.000000  145.000000    4.000000   11.000000   \n",
      "50%      6.000000    0.0   67.000000  170.000000    5.000000   13.000000   \n",
      "75%      6.000000    0.0   70.000000  198.500000    5.000000   16.000000   \n",
      "max      9.000000    0.0   77.000000  350.000000    5.000000   20.000000   \n",
      "\n",
      "              FUN        DEP       SLEEP  \n",
      "count  174.000000  96.000000  197.000000  \n",
      "mean     9.885057   7.166667   12.842640  \n",
      "std      3.218176   4.619562    3.829345  \n",
      "min      5.000000   4.000000    4.000000  \n",
      "25%      7.000000   4.000000   10.000000  \n",
      "50%     10.000000   5.000000   13.000000  \n",
      "75%     12.000000   8.000000   16.000000  \n",
      "max     17.000000  20.000000   20.000000  \n",
      "\n",
      "[8 rows x 60 columns]\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# show column distributions\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DUR</th>\n",
       "      <th>FREQ</th>\n",
       "      <th>NRS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>PIDAY</th>\n",
       "      <th>PIWORK</th>\n",
       "      <th>PPISOC</th>\n",
       "      <th>PICHOR</th>\n",
       "      <th>LBS</th>\n",
       "      <th>LBST</th>\n",
       "      <th>FUS</th>\n",
       "      <th>OPI</th>\n",
       "      <th>INJ</th>\n",
       "      <th>EXE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>18</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  DUR  FREQ  NRS  RAD  PIDAY  PIWORK  PPISOC  PICHOR  LBS  LBST  FUS  \\\n",
       "0    5    1     2  6.0  0.0      3       4     4.0     3.0    0   NaN  NaN   \n",
       "1    8    4     2  3.0  1.0      2       2     3.0     3.0    0   NaN  NaN   \n",
       "2   10    3     3  2.0  1.0      2       2     1.0     2.0    0   NaN  NaN   \n",
       "3   11    4     2  7.0  1.0      4       4     4.0     4.0    0   NaN  NaN   \n",
       "4   12    3     3  6.0  1.0      3       4     2.0     3.0    0   NaN  NaN   \n",
       "5   13    3     3  7.0  1.0      4       4     4.0     4.0    0   NaN  NaN   \n",
       "6   14    3     1  2.0  0.0      1       2     1.0     2.0    0   NaN  NaN   \n",
       "7   15    4     2  1.0  0.0      2       1     1.0     2.0    0   NaN  NaN   \n",
       "8   16    3     2  5.0  0.0      3       3     3.0     3.0    0   NaN  NaN   \n",
       "9   17    4     2  4.0  0.0      4       4     3.0     4.0    0   NaN  NaN   \n",
       "10  18    4     2  5.0  1.0      5       5     5.0     5.0    2   3.0  0.0   \n",
       "\n",
       "    OPI  INJ  EXE  \n",
       "0   1.0  1.0  1.0  \n",
       "1   0.0  0.0  1.0  \n",
       "2   0.0  1.0  1.0  \n",
       "3   0.0  0.0  1.0  \n",
       "4   0.0  0.0  0.0  \n",
       "5   0.0  0.0  1.0  \n",
       "6   NaN  1.0  1.0  \n",
       "7   1.0  NaN  1.0  \n",
       "8   0.0  0.0  NaN  \n",
       "9   0.0  0.0  1.0  \n",
       "10  1.0  1.0  1.0  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dislay\n",
    "df.ix[0:10, 0:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop columns with a high number of NA values, or have the same answer for most questions\n",
    "\n",
    "These columns likely correlate to questions with few resonses. See legend above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DUR</th>\n",
       "      <th>FREQ</th>\n",
       "      <th>NRS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>PIDAY</th>\n",
       "      <th>PIWORK</th>\n",
       "      <th>PPISOC</th>\n",
       "      <th>PICHOR</th>\n",
       "      <th>LBS</th>\n",
       "      <th>...</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EMP</th>\n",
       "      <th>EDU</th>\n",
       "      <th>HT</th>\n",
       "      <th>WT</th>\n",
       "      <th>RACE</th>\n",
       "      <th>PI</th>\n",
       "      <th>FUN</th>\n",
       "      <th>DEP</th>\n",
       "      <th>SLEEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>69.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>67.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>62.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>74.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>64.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>66.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>64.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>68.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>68.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  DUR  FREQ  NRS  RAD  PIDAY  PIWORK  PPISOC  PICHOR  LBS  ...    SEX  \\\n",
       "0   5    1     2  6.0  0.0      3       4     4.0     3.0    0  ...    0.0   \n",
       "1   8    4     2  3.0  1.0      2       2     3.0     3.0    0  ...    0.0   \n",
       "2  10    3     3  2.0  1.0      2       2     1.0     2.0    0  ...    1.0   \n",
       "3  11    4     2  7.0  1.0      4       4     4.0     4.0    0  ...    1.0   \n",
       "4  12    3     3  6.0  1.0      3       4     2.0     3.0    0  ...    0.0   \n",
       "5  13    3     3  7.0  1.0      4       4     4.0     4.0    0  ...    1.0   \n",
       "6  14    3     1  2.0  0.0      1       2     1.0     2.0    0  ...    1.0   \n",
       "7  15    4     2  1.0  0.0      2       1     1.0     2.0    0  ...    1.0   \n",
       "8  16    3     2  5.0  0.0      3       3     3.0     3.0    0  ...    1.0   \n",
       "9  17    4     2  4.0  0.0      4       4     3.0     4.0    0  ...    0.0   \n",
       "\n",
       "   EMP  EDU    HT     WT  RACE    PI   FUN   DEP  SLEEP  \n",
       "0    1    0  72.0  123.0   5.0  14.0  11.0  20.0   11.0  \n",
       "1    1    6  69.0  175.0   5.0  10.0   7.0  12.0    9.0  \n",
       "2    1    8  67.0  130.0   5.0   7.0   5.0  10.0   10.0  \n",
       "3    1    6  62.0  108.0   5.0  16.0  12.0  18.0   17.0  \n",
       "4    1    4  74.0  260.0   5.0  12.0  12.0   7.0   13.0  \n",
       "5    1    7  64.0  170.0   5.0  16.0  13.0   6.0   12.0  \n",
       "6    1    7  66.0  198.0   5.0   6.0   NaN   5.0   13.0  \n",
       "7    1    3  64.0  125.0   5.0   6.0   NaN   7.0    7.0  \n",
       "8    1    7  68.0  185.0   5.0  12.0   NaN   NaN    6.0  \n",
       "9    1    9  68.0  190.0   5.0  15.0   9.0   9.0   10.0  \n",
       "\n",
       "[10 rows x 48 columns]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.drop(['HIS','NAT','ASA','BL','PAC','W','UNK','NA.','LBST','FUS','BL','PAC','W','UNK','NA.','SMOK'], axis=1)\n",
    "df=df.drop(['SAHELP'], axis=1)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set remaining NA values to a default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'DUR', 'FREQ', 'NRS', 'RAD', 'PIDAY', 'PIWORK', 'PPISOC',\n",
       "       'PICHOR', 'LBS', 'OPI', 'INJ', 'EXE', 'PSY', 'UNEMP', 'DIS', 'ABD',\n",
       "       'JOI', 'HEA', 'FIB', 'CHOR', 'STAIR', 'W15', 'ERANDS', 'WORTHL',\n",
       "       'HELPL', 'DEPRES', 'HOPEL', 'SLEEPQ', 'SREFR', 'SPROB', 'SONSET',\n",
       "       'CAT.SAFE', 'CAT.NEVER', 'LIT', 'AS', 'ETOH', 'AGE', 'SEX', 'EMP',\n",
       "       'EDU', 'HT', 'WT', 'RACE', 'PI', 'FUN', 'DEP', 'SLEEP'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DUR</th>\n",
       "      <th>FREQ</th>\n",
       "      <th>NRS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>PIDAY</th>\n",
       "      <th>PIWORK</th>\n",
       "      <th>PPISOC</th>\n",
       "      <th>PICHOR</th>\n",
       "      <th>LBS</th>\n",
       "      <th>...</th>\n",
       "      <th>SEX</th>\n",
       "      <th>EMP</th>\n",
       "      <th>EDU</th>\n",
       "      <th>HT</th>\n",
       "      <th>WT</th>\n",
       "      <th>RACE</th>\n",
       "      <th>PI</th>\n",
       "      <th>FUN</th>\n",
       "      <th>DEP</th>\n",
       "      <th>SLEEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>72.0</td>\n",
       "      <td>123.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>69.0</td>\n",
       "      <td>175.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>67.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>62.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>17.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>74.0</td>\n",
       "      <td>260.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>64.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>12.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>66.0</td>\n",
       "      <td>198.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>64.0</td>\n",
       "      <td>125.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>68.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>68.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  DUR  FREQ  NRS  RAD  PIDAY  PIWORK  PPISOC  PICHOR  LBS  ...    SEX  \\\n",
       "0   5    1     2  6.0  0.0      3       4     4.0     3.0    0  ...    0.0   \n",
       "1   8    4     2  3.0  1.0      2       2     3.0     3.0    0  ...    0.0   \n",
       "2  10    3     3  2.0  1.0      2       2     1.0     2.0    0  ...    1.0   \n",
       "3  11    4     2  7.0  1.0      4       4     4.0     4.0    0  ...    1.0   \n",
       "4  12    3     3  6.0  1.0      3       4     2.0     3.0    0  ...    0.0   \n",
       "5  13    3     3  7.0  1.0      4       4     4.0     4.0    0  ...    1.0   \n",
       "6  14    3     1  2.0  0.0      1       2     1.0     2.0    0  ...    1.0   \n",
       "7  15    4     2  1.0  0.0      2       1     1.0     2.0    0  ...    1.0   \n",
       "8  16    3     2  5.0  0.0      3       3     3.0     3.0    0  ...    1.0   \n",
       "9  17    4     2  4.0  0.0      4       4     3.0     4.0    0  ...    0.0   \n",
       "\n",
       "   EMP  EDU    HT     WT  RACE    PI   FUN   DEP  SLEEP  \n",
       "0    1    0  72.0  123.0   5.0  14.0  11.0  20.0   11.0  \n",
       "1    1    6  69.0  175.0   5.0  10.0   7.0  12.0    9.0  \n",
       "2    1    8  67.0  130.0   5.0   7.0   5.0  10.0   10.0  \n",
       "3    1    6  62.0  108.0   5.0  16.0  12.0  18.0   17.0  \n",
       "4    1    4  74.0  260.0   5.0  12.0  12.0   7.0   13.0  \n",
       "5    1    7  64.0  170.0   5.0  16.0  13.0   6.0   12.0  \n",
       "6    1    7  66.0  198.0   5.0   6.0   0.0   5.0   13.0  \n",
       "7    1    3  64.0  125.0   5.0   6.0   0.0   7.0    7.0  \n",
       "8    1    7  68.0  185.0   5.0  12.0   0.0   0.0    6.0  \n",
       "9    1    9  68.0  190.0   5.0  15.0   9.0   9.0   10.0  \n",
       "\n",
       "[10 rows x 48 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# set remaining NA to 0\n",
    "df.fillna(0, inplace=True)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define data types\n",
    "\n",
    "Ensure that survey attribures are interpreted by the correct datat type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['ID']=df['ID']\n",
    "df['DUR']=df['DUR'].astype('int')\n",
    "df['FREQ']=df['FREQ'].astype('int')\n",
    "df['NRS']=df['NRS'].astype('int')\n",
    "df['RAD']=df['RAD'].astype('category')\n",
    "\n",
    "df['PIDAY']=df['PIDAY'].astype('int')\n",
    "df['PIWORK']=df['PIWORK'].astype('int')\n",
    "df['PPISOC']=df['PPISOC'].astype('int')\n",
    "df['PICHOR']=df['PICHOR'].astype('int')\n",
    "\n",
    "df['LBS']=df['LBS'].astype('category')\n",
    "df['`']=df['OPI'].astype('category')\n",
    "df['INJ']=df['INJ'].astype('category')\n",
    "df['EXE']=df['EXE'].astype('category')\n",
    "df['PSY']=df['PSY'].astype('category')\n",
    "df['UNEMP']=df['UNEMP'].astype('category')\n",
    "df['DIS']=df['DIS'].astype('category')\n",
    "df['ABD']=df['ABD'].astype('int')\n",
    "df['JOI']=df['JOI'].astype('int')\n",
    "df['HEA']=df['HEA'].astype('int')\n",
    "df['FIB']=df['FIB'].astype('int')\n",
    "df['CHOR']=df['CHOR'].astype('int')\n",
    "df['STAIR']=df['STAIR'].astype('int')\n",
    "df['W15']=df['W15'].astype('int')\n",
    "df['ERANDS']=df['ERANDS'].astype('int')\n",
    "df['WORTHL']=df['WORTHL'].astype('int')\n",
    "df['HELPL']=df['HELPL'].astype('int')\n",
    "df['DEPRES']=df['DEPRES'].astype('int')\n",
    "df['HOPEL']=df['HOPEL'].astype('int')\n",
    "df['SLEEPQ']=df['SLEEPQ'].astype('int')\n",
    "df['SREFR']=df['SREFR'].astype('int')\n",
    "df['SPROB']=df['SPROB'].astype('int')\n",
    "df['SONSET']=df['SONSET'].astype('int')\n",
    "df['CAT.SAFE']=df['CAT.SAFE'].astype('int')\n",
    "df['CAT.NEVER']=df['CAT.NEVER'].astype('int')\n",
    "df['LIT']=df['LIT'].astype('int')\n",
    "df['AS']=df['AS'].astype('int')\n",
    "df['ETOH']=df['ETOH'].astype('int')\n",
    "df['AGE']=df['AGE'].astype('int')\n",
    "df['SEX']=df['SEX'].astype('category')\n",
    "df['EMP']=df['EMP'].astype('category')\n",
    "df['EDU']=df['EDU'].astype('int')\n",
    "df['HT']=df['HT'].astype('int')\n",
    "df['WT']=df['WT'].astype('int')\n",
    "df['RACE']=df['RACE'].astype('category')\n",
    "df['PI']=df['PI'].astype('int')\n",
    "df['FUN']=df['FUN'].astype('int')\n",
    "df['DEP']=df['DEP'].astype('int')\n",
    "df['SLEEP']=df['SLEEP'].astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>DUR</th>\n",
       "      <th>FREQ</th>\n",
       "      <th>NRS</th>\n",
       "      <th>PIDAY</th>\n",
       "      <th>PIWORK</th>\n",
       "      <th>PPISOC</th>\n",
       "      <th>PICHOR</th>\n",
       "      <th>OPI</th>\n",
       "      <th>ABD</th>\n",
       "      <th>...</th>\n",
       "      <th>AS</th>\n",
       "      <th>ETOH</th>\n",
       "      <th>AGE</th>\n",
       "      <th>EDU</th>\n",
       "      <th>HT</th>\n",
       "      <th>WT</th>\n",
       "      <th>PI</th>\n",
       "      <th>FUN</th>\n",
       "      <th>DEP</th>\n",
       "      <th>SLEEP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.00000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>200.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>115.520000</td>\n",
       "      <td>3.530000</td>\n",
       "      <td>2.45000</td>\n",
       "      <td>5.465000</td>\n",
       "      <td>3.330000</td>\n",
       "      <td>3.430000</td>\n",
       "      <td>3.145000</td>\n",
       "      <td>3.390000</td>\n",
       "      <td>0.580000</td>\n",
       "      <td>0.490000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.460000</td>\n",
       "      <td>0.485000</td>\n",
       "      <td>44.950000</td>\n",
       "      <td>4.905000</td>\n",
       "      <td>62.505000</td>\n",
       "      <td>164.430000</td>\n",
       "      <td>13.175000</td>\n",
       "      <td>8.600000</td>\n",
       "      <td>3.440000</td>\n",
       "      <td>12.650000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>68.958436</td>\n",
       "      <td>0.686627</td>\n",
       "      <td>0.72118</td>\n",
       "      <td>2.147518</td>\n",
       "      <td>1.061477</td>\n",
       "      <td>1.091355</td>\n",
       "      <td>1.253728</td>\n",
       "      <td>1.176786</td>\n",
       "      <td>0.494797</td>\n",
       "      <td>0.633963</td>\n",
       "      <td>...</td>\n",
       "      <td>0.781957</td>\n",
       "      <td>0.890923</td>\n",
       "      <td>14.929567</td>\n",
       "      <td>2.106695</td>\n",
       "      <td>17.062109</td>\n",
       "      <td>58.187231</td>\n",
       "      <td>4.353059</td>\n",
       "      <td>4.484479</td>\n",
       "      <td>4.803307</td>\n",
       "      <td>4.109983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>57.750000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.00000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>63.750000</td>\n",
       "      <td>140.000000</td>\n",
       "      <td>10.750000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>107.500000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>66.000000</td>\n",
       "      <td>165.000000</td>\n",
       "      <td>13.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>12.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>172.250000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>56.250000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>197.250000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>16.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>242.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>81.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>350.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 38 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID         DUR       FREQ         NRS       PIDAY      PIWORK  \\\n",
       "count  200.000000  200.000000  200.00000  200.000000  200.000000  200.000000   \n",
       "mean   115.520000    3.530000    2.45000    5.465000    3.330000    3.430000   \n",
       "std     68.958436    0.686627    0.72118    2.147518    1.061477    1.091355   \n",
       "min      5.000000    1.000000    1.00000    0.000000    1.000000    1.000000   \n",
       "25%     57.750000    3.000000    2.00000    4.000000    3.000000    3.000000   \n",
       "50%    107.500000    4.000000    3.00000    6.000000    3.000000    3.000000   \n",
       "75%    172.250000    4.000000    3.00000    7.000000    4.000000    4.000000   \n",
       "max    242.000000    4.000000    3.00000   10.000000    5.000000    5.000000   \n",
       "\n",
       "           PPISOC      PICHOR         OPI         ABD     ...              AS  \\\n",
       "count  200.000000  200.000000  200.000000  200.000000     ...      200.000000   \n",
       "mean     3.145000    3.390000    0.580000    0.490000     ...        0.460000   \n",
       "std      1.253728    1.176786    0.494797    0.633963     ...        0.781957   \n",
       "min      0.000000    0.000000    0.000000    0.000000     ...        0.000000   \n",
       "25%      2.000000    2.000000    0.000000    0.000000     ...        0.000000   \n",
       "50%      3.000000    4.000000    1.000000    0.000000     ...        0.000000   \n",
       "75%      4.000000    4.000000    1.000000    1.000000     ...        1.000000   \n",
       "max      5.000000    5.000000    1.000000    2.000000     ...        3.000000   \n",
       "\n",
       "             ETOH         AGE         EDU          HT          WT          PI  \\\n",
       "count  200.000000  200.000000  200.000000  200.000000  200.000000  200.000000   \n",
       "mean     0.485000   44.950000    4.905000   62.505000  164.430000   13.175000   \n",
       "std      0.890923   14.929567    2.106695   17.062109   58.187231    4.353059   \n",
       "min      0.000000    0.000000    0.000000    0.000000    0.000000    0.000000   \n",
       "25%      0.000000   35.000000    3.000000   63.750000  140.000000   10.750000   \n",
       "50%      0.000000   46.000000    6.000000   66.000000  165.000000   13.000000   \n",
       "75%      1.000000   56.250000    6.000000   70.000000  197.250000   16.000000   \n",
       "max      3.000000   81.000000    9.000000   77.000000  350.000000   20.000000   \n",
       "\n",
       "              FUN         DEP       SLEEP  \n",
       "count  200.000000  200.000000  200.000000  \n",
       "mean     8.600000    3.440000   12.650000  \n",
       "std      4.484479    4.803307    4.109983  \n",
       "min      0.000000    0.000000    0.000000  \n",
       "25%      6.000000    0.000000   10.000000  \n",
       "50%      9.000000    0.000000   12.000000  \n",
       "75%     12.000000    5.000000   16.000000  \n",
       "max     17.000000   20.000000   20.000000  \n",
       "\n",
       "[8 rows x 38 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select relevant subset of features \n",
    "\n",
    "Select a subset of features that have been found to have relatively high predictive value based on using recurisve feature elimination that also captures physical function, pain interference, and pain intensity, along with opiod use radiating pain and demographics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DUR</th>\n",
       "      <th>FREQ</th>\n",
       "      <th>NRS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>JOI</th>\n",
       "      <th>LBS</th>\n",
       "      <th>PICHOR</th>\n",
       "      <th>OPI</th>\n",
       "      <th>INJ</th>\n",
       "      <th>EXE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>DEPRES</th>\n",
       "      <th>SEX</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   DUR  FREQ  NRS  RAD  JOI LBS  PICHOR  OPI  INJ  EXE  DIS  DEPRES  SEX\n",
       "0    1     2    6  0.0    2   0       3  1.0  1.0  1.0  1.0       5  0.0\n",
       "1    4     2    3  1.0    1   0       3  0.0  0.0  1.0  0.0       4  0.0\n",
       "2    3     3    2  1.0    0   0       2  0.0  1.0  1.0  0.0       4  1.0\n",
       "3    4     2    7  1.0    2   0       4  0.0  0.0  1.0  0.0       5  1.0\n",
       "4    3     3    6  1.0    2   0       3  0.0  0.0  0.0  0.0       2  0.0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df[['FREQ', 'RAD', 'INJ', 'EXE', 'DIS', 'JOI', 'CHOR', 'ERANDS', 'LIT', 'SEX', 'OPI']].head()\n",
    "\n",
    "#data = df[['DUR', 'FREQ', 'NRS', 'RAD', 'JOI', 'LBS', 'PICHOR', 'OPI', 'INJ', 'EXE', 'DIS', 'DEPRES', 'AGE', 'SEX']]\n",
    "# skip age\n",
    "data = df[['DUR', 'FREQ', 'NRS', 'RAD', 'JOI', 'LBS', 'PICHOR', 'OPI', 'INJ', 'EXE', 'DIS', 'DEPRES', 'SEX']]\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Learning Bayesian Networks\n",
    "\n",
    "\n",
    "Previous notebooks showed how Bayesian networks economically encode a probability distribution over a set of variables, and how they can be used e.g. to predict variable states, or to generate new samples from the joint distribution. This section will be about obtaining a Bayesian network, given a set of sample data. Learning a Bayesian network can be split into two problems:\n",
    "\n",
    " **Parameter learning:** Given a set of data samples and a DAG that captures the dependencies between the variables, estimate the (conditional) probability distributions of the individual variables.\n",
    " \n",
    " **Structure learning:** Given a set of data samples, estimate a DAG that captures the dependencies between the variables.\n",
    " \n",
    "This notebook aims to illustrate how parameter learning and structure learning can be done with pgmpy.\n",
    "Currently, the library supports:\n",
    " - Parameter learning for *discrete* nodes:\n",
    "   - Maximum Likelihood Estimation\n",
    "   - Bayesian Estimation\n",
    " - Structure learning for *discrete*, *fully observed* networks:\n",
    "   - Score-based structure estimation (BIC/BDeu/K2 score; exhaustive search, hill climb/tabu search)\n",
    "   - Constraint-based structure estimation (PC)\n",
    "   - Hybrid structure estimation (MMHC)\n",
    "\n",
    "\n",
    "## Parameter Learning\n",
    "\n",
    "Suppose we have the following data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assume the following Bayesian network is provided. Naive Bayes network assuming all variables are conditioanlly indepedent givin OPI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pgmpy.models import BayesianModel\n",
    "\n",
    "network = [('OPI','DUR'), ('OPI','FREQ'), ('OPI','NRS'), ('OPI','RAD'), ('OPI','JOI'), ('OPI','LBS'), ('OPI','PICHOR'), ('OPI','INJ'), ('OPI','EXE'), ('OPI','DIS'), ('OPI','DEPRES'), ('OPI','SEX')]   \n",
    "\n",
    "model = BayesianModel(network)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parameter learning is the task to estimate the values of the conditional probability distributions (CPDs), for the variables `fruit`, `size`, and `tasty`. \n",
    "\n",
    "#### State counts\n",
    "To make sense of the given data, we can start by counting how often each state of the variable occurs. If the variable is dependent on parents, the counts are done conditionally on the parents states, i.e. for seperately for each parent configuration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      OPI\n",
      "0.0   84\n",
      "1.0  116\n",
      "\n",
      " OPI   0.0  1.0\n",
      "FREQ          \n",
      "1      15   12\n",
      "2      22   34\n",
      "3      47   70\n",
      "\n",
      " OPI   0.0   1.0\n",
      "NRS            \n",
      "0     1.0   0.0\n",
      "1     0.0   3.0\n",
      "2     8.0   6.0\n",
      "3    12.0  13.0\n",
      "4    14.0  13.0\n",
      "5    11.0  15.0\n",
      "6    11.0  21.0\n",
      "7    19.0  20.0\n",
      "8     6.0  12.0\n",
      "9     1.0   9.0\n",
      "10    1.0   4.0\n",
      "\n",
      " OPI  0.0  1.0\n",
      "RAD          \n",
      "0.0   57   59\n",
      "1.0   27   57\n",
      "\n",
      " OPI  0.0  1.0\n",
      "JOI          \n",
      "0     22   21\n",
      "1     39   45\n",
      "2     23   50\n",
      "\n",
      " OPI  0.0  1.0\n",
      "LBS          \n",
      "0     80   92\n",
      "1      1   15\n",
      "2      3    9\n",
      "\n",
      " OPI      0.0   1.0\n",
      "PICHOR            \n",
      "0        0.0   1.0\n",
      "1        6.0   4.0\n",
      "2       19.0  21.0\n",
      "3       24.0  22.0\n",
      "4       26.0  39.0\n",
      "5        9.0  29.0\n",
      "\n",
      " OPI  0.0  1.0\n",
      "INJ          \n",
      "0.0   71   56\n",
      "1.0   13   60\n",
      "\n",
      " OPI  0.0  1.0\n",
      "EXE          \n",
      "0.0   27   10\n",
      "1.0   57  106\n",
      "\n",
      " OPI  0.0  1.0\n",
      "DIS          \n",
      "0.0   82   88\n",
      "1.0    2   28\n",
      "\n",
      " OPI     0.0  1.0\n",
      "DEPRES          \n",
      "0        25   39\n",
      "1        19   22\n",
      "2        27   23\n",
      "4        10   22\n",
      "5         3   10\n",
      "\n",
      " OPI  0.0  1.0\n",
      "SEX          \n",
      "0.0   47   51\n",
      "1.0   37   65\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import ParameterEstimator\n",
    "pe = ParameterEstimator(model, data)\n",
    "print(\"\\n\", pe.state_counts('OPI'))  # unconditional\n",
    "print(\"\\n\", pe.state_counts('FREQ'))  # conditional on OPI\n",
    "print(\"\\n\", pe.state_counts('NRS'))  # conditional on OPI\n",
    "print(\"\\n\", pe.state_counts('RAD'))  # conditional on OPI\n",
    "print(\"\\n\", pe.state_counts('JOI'))  # conditional on OPI\n",
    "print(\"\\n\", pe.state_counts('LBS'))  # conditional on OPI\n",
    "print(\"\\n\", pe.state_counts('PICHOR'))  # conditional on OPI\n",
    "print(\"\\n\", pe.state_counts('INJ'))  # conditional on OPI\n",
    "print(\"\\n\", pe.state_counts('EXE'))  # conditional on OPI\n",
    "print(\"\\n\", pe.state_counts('DIS'))  # conditional on OPI\n",
    "print(\"\\n\", pe.state_counts('DEPRES'))  # conditional on OPI\n",
    "print(\"\\n\", pe.state_counts('SEX'))  # conditional on OPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see, for example, that as many apples as bananas were observed and that `5` large bananas were tasty, while only `1` was not.\n",
    "\n",
    "#### Maximum Likelihood Estimation\n",
    "\n",
    "A natural estimate for the CPDs is to simply use the *relative frequencies*, with which the variable states have occured. \n",
    "\n",
    "This approach is *Maximum Likelihood Estimation (MLE)*. According to MLE, we should fill the CPDs in such a way, that $P(\\text{data}|\\text{model})$ is maximal. This is achieved when using the *relative frequencies*. See [1], section 17.1 for an introduction to ML parameter estimation. pgmpy supports MLE as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ╒══════════╤══════╕\n",
      "│ OPI(0.0) │ 0.42 │\n",
      "├──────────┼──────┤\n",
      "│ OPI(1.0) │ 0.58 │\n",
      "╘══════════╧══════╛\n",
      "\n",
      " ╒═════════╤═════════════════════╤═════════════════════╕\n",
      "│ OPI     │ OPI(0.0)            │ OPI(1.0)            │\n",
      "├─────────┼─────────────────────┼─────────────────────┤\n",
      "│ FREQ(1) │ 0.17857142857142858 │ 0.10344827586206896 │\n",
      "├─────────┼─────────────────────┼─────────────────────┤\n",
      "│ FREQ(2) │ 0.2619047619047619  │ 0.29310344827586204 │\n",
      "├─────────┼─────────────────────┼─────────────────────┤\n",
      "│ FREQ(3) │ 0.5595238095238095  │ 0.603448275862069   │\n",
      "╘═════════╧═════════════════════╧═════════════════════╛\n",
      "\n",
      " ╒═════════╤══════════════════════╤══════════════════════╕\n",
      "│ OPI     │ OPI(0.0)             │ OPI(1.0)             │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(0)  │ 0.011904761904761904 │ 0.0                  │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(1)  │ 0.0                  │ 0.02586206896551724  │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(2)  │ 0.09523809523809523  │ 0.05172413793103448  │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(3)  │ 0.14285714285714285  │ 0.11206896551724138  │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(4)  │ 0.16666666666666666  │ 0.11206896551724138  │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(5)  │ 0.13095238095238096  │ 0.12931034482758622  │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(6)  │ 0.13095238095238096  │ 0.1810344827586207   │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(7)  │ 0.2261904761904762   │ 0.1724137931034483   │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(8)  │ 0.07142857142857142  │ 0.10344827586206896  │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(9)  │ 0.011904761904761904 │ 0.07758620689655173  │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(10) │ 0.011904761904761904 │ 0.034482758620689655 │\n",
      "╘═════════╧══════════════════════╧══════════════════════╛\n",
      "\n",
      " ╒══════════╤═════════════════════╤═════════════════════╕\n",
      "│ OPI      │ OPI(0.0)            │ OPI(1.0)            │\n",
      "├──────────┼─────────────────────┼─────────────────────┤\n",
      "│ RAD(0.0) │ 0.6785714285714286  │ 0.5086206896551724  │\n",
      "├──────────┼─────────────────────┼─────────────────────┤\n",
      "│ RAD(1.0) │ 0.32142857142857145 │ 0.49137931034482757 │\n",
      "╘══════════╧═════════════════════╧═════════════════════╛\n",
      "\n",
      " ╒════════╤═════════════════════╤═════════════════════╕\n",
      "│ OPI    │ OPI(0.0)            │ OPI(1.0)            │\n",
      "├────────┼─────────────────────┼─────────────────────┤\n",
      "│ JOI(0) │ 0.2619047619047619  │ 0.1810344827586207  │\n",
      "├────────┼─────────────────────┼─────────────────────┤\n",
      "│ JOI(1) │ 0.4642857142857143  │ 0.3879310344827586  │\n",
      "├────────┼─────────────────────┼─────────────────────┤\n",
      "│ JOI(2) │ 0.27380952380952384 │ 0.43103448275862066 │\n",
      "╘════════╧═════════════════════╧═════════════════════╛\n",
      "\n",
      " ╒════════╤══════════════════════╤═════════════════════╕\n",
      "│ OPI    │ OPI(0.0)             │ OPI(1.0)            │\n",
      "├────────┼──────────────────────┼─────────────────────┤\n",
      "│ LBS(0) │ 0.9523809523809523   │ 0.7931034482758621  │\n",
      "├────────┼──────────────────────┼─────────────────────┤\n",
      "│ LBS(1) │ 0.011904761904761904 │ 0.12931034482758622 │\n",
      "├────────┼──────────────────────┼─────────────────────┤\n",
      "│ LBS(2) │ 0.03571428571428571  │ 0.07758620689655173 │\n",
      "╘════════╧══════════════════════╧═════════════════════╛\n",
      "\n",
      " ╒═══════════╤═════════════════════╤══════════════════════╕\n",
      "│ OPI       │ OPI(0.0)            │ OPI(1.0)             │\n",
      "├───────────┼─────────────────────┼──────────────────────┤\n",
      "│ PICHOR(0) │ 0.0                 │ 0.008620689655172414 │\n",
      "├───────────┼─────────────────────┼──────────────────────┤\n",
      "│ PICHOR(1) │ 0.07142857142857142 │ 0.034482758620689655 │\n",
      "├───────────┼─────────────────────┼──────────────────────┤\n",
      "│ PICHOR(2) │ 0.2261904761904762  │ 0.1810344827586207   │\n",
      "├───────────┼─────────────────────┼──────────────────────┤\n",
      "│ PICHOR(3) │ 0.2857142857142857  │ 0.1896551724137931   │\n",
      "├───────────┼─────────────────────┼──────────────────────┤\n",
      "│ PICHOR(4) │ 0.30952380952380953 │ 0.33620689655172414  │\n",
      "├───────────┼─────────────────────┼──────────────────────┤\n",
      "│ PICHOR(5) │ 0.10714285714285714 │ 0.25                 │\n",
      "╘═══════════╧═════════════════════╧══════════════════════╛\n",
      "\n",
      " ╒══════════╤═════════════════════╤════════════════════╕\n",
      "│ OPI      │ OPI(0.0)            │ OPI(1.0)           │\n",
      "├──────────┼─────────────────────┼────────────────────┤\n",
      "│ INJ(0.0) │ 0.8452380952380952  │ 0.4827586206896552 │\n",
      "├──────────┼─────────────────────┼────────────────────┤\n",
      "│ INJ(1.0) │ 0.15476190476190477 │ 0.5172413793103449 │\n",
      "╘══════════╧═════════════════════╧════════════════════╛\n",
      "\n",
      " ╒══════════╤═════════════════════╤═════════════════════╕\n",
      "│ OPI      │ OPI(0.0)            │ OPI(1.0)            │\n",
      "├──────────┼─────────────────────┼─────────────────────┤\n",
      "│ EXE(0.0) │ 0.32142857142857145 │ 0.08620689655172414 │\n",
      "├──────────┼─────────────────────┼─────────────────────┤\n",
      "│ EXE(1.0) │ 0.6785714285714286  │ 0.9137931034482759  │\n",
      "╘══════════╧═════════════════════╧═════════════════════╛\n",
      "\n",
      " ╒══════════╤══════════════════════╤════════════════════╕\n",
      "│ OPI      │ OPI(0.0)             │ OPI(1.0)           │\n",
      "├──────────┼──────────────────────┼────────────────────┤\n",
      "│ DIS(0.0) │ 0.9761904761904762   │ 0.7586206896551724 │\n",
      "├──────────┼──────────────────────┼────────────────────┤\n",
      "│ DIS(1.0) │ 0.023809523809523808 │ 0.2413793103448276 │\n",
      "╘══════════╧══════════════════════╧════════════════════╛\n",
      "\n",
      " ╒═══════════╤═════════════════════╤═════════════════════╕\n",
      "│ OPI       │ OPI(0.0)            │ OPI(1.0)            │\n",
      "├───────────┼─────────────────────┼─────────────────────┤\n",
      "│ DEPRES(0) │ 0.2976190476190476  │ 0.33620689655172414 │\n",
      "├───────────┼─────────────────────┼─────────────────────┤\n",
      "│ DEPRES(1) │ 0.2261904761904762  │ 0.1896551724137931  │\n",
      "├───────────┼─────────────────────┼─────────────────────┤\n",
      "│ DEPRES(2) │ 0.32142857142857145 │ 0.19827586206896552 │\n",
      "├───────────┼─────────────────────┼─────────────────────┤\n",
      "│ DEPRES(4) │ 0.11904761904761904 │ 0.1896551724137931  │\n",
      "├───────────┼─────────────────────┼─────────────────────┤\n",
      "│ DEPRES(5) │ 0.03571428571428571 │ 0.08620689655172414 │\n",
      "╘═══════════╧═════════════════════╧═════════════════════╛\n",
      "\n",
      " ╒══════════╤═════════════════════╤════════════════════╕\n",
      "│ OPI      │ OPI(0.0)            │ OPI(1.0)           │\n",
      "├──────────┼─────────────────────┼────────────────────┤\n",
      "│ SEX(0.0) │ 0.5595238095238095  │ 0.4396551724137931 │\n",
      "├──────────┼─────────────────────┼────────────────────┤\n",
      "│ SEX(1.0) │ 0.44047619047619047 │ 0.5603448275862069 │\n",
      "╘══════════╧═════════════════════╧════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import MaximumLikelihoodEstimator\n",
    "mle = MaximumLikelihoodEstimator(model, data)\n",
    "print(\"\\n\", mle.estimate_cpd('OPI'))  # unconditional\n",
    "print(\"\\n\", mle.estimate_cpd('FREQ'))  # conditional on OPI\n",
    "print(\"\\n\", mle.estimate_cpd('NRS'))  # conditional on OPI\n",
    "print(\"\\n\", mle.estimate_cpd('RAD'))  # conditional on OPI\n",
    "print(\"\\n\", mle.estimate_cpd('JOI'))  # conditional on OPI\n",
    "print(\"\\n\", mle.estimate_cpd('LBS'))  # conditional on OPI\n",
    "print(\"\\n\", mle.estimate_cpd('PICHOR'))  # conditional on OPI\n",
    "print(\"\\n\", mle.estimate_cpd('INJ'))  # conditional on OPI\n",
    "print(\"\\n\", mle.estimate_cpd('EXE'))  # conditional on OPI\n",
    "print(\"\\n\", mle.estimate_cpd('DIS'))  # conditional on OPI\n",
    "print(\"\\n\", mle.estimate_cpd('DEPRES'))  # conditional on OPI\n",
    "print(\"\\n\", mle.estimate_cpd('SEX'))  # conditional on OPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`mle.estimate_cpd(variable)` computes the state counts and divides each cell by the (conditional) sample size. The `mle.get_parameters()`-method returns a list of CPDs for all variable of the model.\n",
    "\n",
    "The built-in `fit()`-method of `BayesianModel` provides more convenient access to parameter estimators:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calibrate all CPDs of `model` using MLE:\n",
    "model.fit(data, estimator=MaximumLikelihoodEstimator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "While very straightforward, the ML estimator has the problem of *overfitting* to the data. We may not have enough observations to rely on the observed frequencies. If the observed data is not representative for the underlying distribution, ML estimations will be extremly far off. \n",
    "\n",
    "When estimating parameters for Bayesian networks, lack of data is a frequent problem. Even if the total sample size is very large, the fact that state counts are done conditionally for each parents configuration causes immense fragmentation. If a variable has 3 parents that can each take 10 states, then state counts will be done seperately for `10^3 = 1000` parents configurations. This makes MLE very fragile and unstable for learning Bayesian Network parameters. A way to mitigate MLE's overfitting is *Bayesian Parameter Estimation*.\n",
    "\n",
    "#### Bayesian Parameter Estimation\n",
    "\n",
    "The Bayesian Parameter Estimator starts with already existing prior CPDs, that express our beliefs about the variables *before* the data was observed. Those \"priors\" are then updated, using the state counts from the observed data. See [1], Section 17.3 for a general introduction to Bayesian estimators.\n",
    "\n",
    "One can think of the priors as consisting in *pseudo state counts*, that are added to the actual counts before normalization.\n",
    "Unless one wants to encode specific beliefs about the distributions of the variables, one commonly chooses uniform priors, i.e. ones that deem all states equiprobable.\n",
    "\n",
    "A very simple prior is the so-called *K2* prior, which simply adds `1` to the count of every single state.\n",
    "A somewhat more sensible choice of prior is *BDeu* (Bayesian Dirichlet equivalent uniform prior). For BDeu we need to specify an *equivalent sample size* `N` and then the pseudo-counts are the equivalent of having observed `N` uniform samples of each variable (and each parent configuration). In pgmpy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ╒══════════╤══════════╕\n",
      "│ OPI(0.0) │ 0.427273 │\n",
      "├──────────┼──────────┤\n",
      "│ OPI(1.0) │ 0.572727 │\n",
      "╘══════════╧══════════╛\n",
      "\n",
      " ╒═════════╤═════════════════════╤═════════════════════╕\n",
      "│ OPI     │ OPI(0.0)            │ OPI(1.0)            │\n",
      "├─────────┼─────────────────────┼─────────────────────┤\n",
      "│ FREQ(1) │ 0.19503546099290778 │ 0.1216931216931217  │\n",
      "├─────────┼─────────────────────┼─────────────────────┤\n",
      "│ FREQ(2) │ 0.2695035460992908  │ 0.29629629629629634 │\n",
      "├─────────┼─────────────────────┼─────────────────────┤\n",
      "│ FREQ(3) │ 0.5354609929078015  │ 0.582010582010582   │\n",
      "╘═════════╧═════════════════════╧═════════════════════╛\n",
      "\n",
      " ╒═════════╤══════════════════════╤══════════════════════╕\n",
      "│ OPI     │ OPI(0.0)             │ OPI(1.0)             │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(0)  │ 0.020309477756286273 │ 0.007215007215007216 │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(1)  │ 0.009671179883945842 │ 0.031024531024531028 │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(2)  │ 0.09477756286266925  │ 0.05483405483405484  │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(3)  │ 0.13733075435203096  │ 0.1103896103896104   │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(4)  │ 0.1586073500967118   │ 0.1103896103896104   │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(5)  │ 0.12669245647969055  │ 0.12626262626262627  │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(6)  │ 0.12669245647969055  │ 0.17388167388167391  │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(7)  │ 0.21179883945841396  │ 0.16594516594516598  │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(8)  │ 0.07350096711798841  │ 0.10245310245310246  │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(9)  │ 0.020309477756286273 │ 0.07864357864357864  │\n",
      "├─────────┼──────────────────────┼──────────────────────┤\n",
      "│ NRS(10) │ 0.020309477756286273 │ 0.03896103896103897  │\n",
      "╘═════════╧══════════════════════╧══════════════════════╛\n",
      "\n",
      " ╒══════════╤════════════════════╤═════════════════════╕\n",
      "│ OPI      │ OPI(0.0)           │ OPI(1.0)            │\n",
      "├──────────┼────────────────────┼─────────────────────┤\n",
      "│ RAD(0.0) │ 0.6595744680851063 │ 0.5079365079365079  │\n",
      "├──────────┼────────────────────┼─────────────────────┤\n",
      "│ RAD(1.0) │ 0.3404255319148936 │ 0.49206349206349204 │\n",
      "╘══════════╧════════════════════╧═════════════════════╛\n",
      "\n",
      " ╒════════╤═════════════════════╤════════════════════╕\n",
      "│ OPI    │ OPI(0.0)            │ OPI(1.0)           │\n",
      "├────────┼─────────────────────┼────────────────────┤\n",
      "│ JOI(0) │ 0.2695035460992908  │ 0.1931216931216931 │\n",
      "├────────┼─────────────────────┼────────────────────┤\n",
      "│ JOI(1) │ 0.45035460992907805 │ 0.3835978835978836 │\n",
      "├────────┼─────────────────────┼────────────────────┤\n",
      "│ JOI(2) │ 0.28014184397163117 │ 0.4232804232804233 │\n",
      "╘════════╧═════════════════════╧════════════════════╛\n",
      "\n",
      " ╒════════╤══════════════════════╤═════════════════════╕\n",
      "│ OPI    │ OPI(0.0)             │ OPI(1.0)            │\n",
      "├────────┼──────────────────────┼─────────────────────┤\n",
      "│ LBS(0) │ 0.8865248226950355   │ 0.7566137566137566  │\n",
      "├────────┼──────────────────────┼─────────────────────┤\n",
      "│ LBS(1) │ 0.046099290780141855 │ 0.14550264550264552 │\n",
      "├────────┼──────────────────────┼─────────────────────┤\n",
      "│ LBS(2) │ 0.06737588652482271  │ 0.0978835978835979  │\n",
      "╘════════╧══════════════════════╧═════════════════════╛\n",
      "\n",
      " ╒═══════════╤══════════════════════╤══════════════════════╕\n",
      "│ OPI       │ OPI(0.0)             │ OPI(1.0)             │\n",
      "├───────────┼──────────────────────┼──────────────────────┤\n",
      "│ PICHOR(0) │ 0.017730496453900707 │ 0.021164021164021163 │\n",
      "├───────────┼──────────────────────┼──────────────────────┤\n",
      "│ PICHOR(1) │ 0.08156028368794326  │ 0.04497354497354497  │\n",
      "├───────────┼──────────────────────┼──────────────────────┤\n",
      "│ PICHOR(2) │ 0.21985815602836878  │ 0.17989417989417988  │\n",
      "├───────────┼──────────────────────┼──────────────────────┤\n",
      "│ PICHOR(3) │ 0.27304964539007087  │ 0.18783068783068782  │\n",
      "├───────────┼──────────────────────┼──────────────────────┤\n",
      "│ PICHOR(4) │ 0.29432624113475175  │ 0.3227513227513227   │\n",
      "├───────────┼──────────────────────┼──────────────────────┤\n",
      "│ PICHOR(5) │ 0.11347517730496452  │ 0.24338624338624337  │\n",
      "╘═══════════╧══════════════════════╧══════════════════════╛\n",
      "\n",
      " ╒══════════╤═════════════════════╤═════════════════════╕\n",
      "│ OPI      │ OPI(0.0)            │ OPI(1.0)            │\n",
      "├──────────┼─────────────────────┼─────────────────────┤\n",
      "│ INJ(0.0) │ 0.8085106382978723  │ 0.48412698412698413 │\n",
      "├──────────┼─────────────────────┼─────────────────────┤\n",
      "│ INJ(1.0) │ 0.19148936170212766 │ 0.5158730158730159  │\n",
      "╘══════════╧═════════════════════╧═════════════════════╛\n",
      "\n",
      " ╒══════════╤════════════════════╤═════════════════════╕\n",
      "│ OPI      │ OPI(0.0)           │ OPI(1.0)            │\n",
      "├──────────┼────────────────────┼─────────────────────┤\n",
      "│ EXE(0.0) │ 0.3404255319148936 │ 0.11904761904761904 │\n",
      "├──────────┼────────────────────┼─────────────────────┤\n",
      "│ EXE(1.0) │ 0.6595744680851063 │ 0.8809523809523809  │\n",
      "╘══════════╧════════════════════╧═════════════════════╛\n",
      "\n",
      " ╒══════════╤═════════════════════╤════════════════════╕\n",
      "│ OPI      │ OPI(0.0)            │ OPI(1.0)           │\n",
      "├──────────┼─────────────────────┼────────────────────┤\n",
      "│ DIS(0.0) │ 0.925531914893617   │ 0.7380952380952381 │\n",
      "├──────────┼─────────────────────┼────────────────────┤\n",
      "│ DIS(1.0) │ 0.07446808510638298 │ 0.2619047619047619 │\n",
      "╘══════════╧═════════════════════╧════════════════════╛\n",
      "\n",
      " ╒═══════════╤═════════════════════╤═════════════════════╕\n",
      "│ OPI       │ OPI(0.0)            │ OPI(1.0)            │\n",
      "├───────────┼─────────────────────┼─────────────────────┤\n",
      "│ DEPRES(0) │ 0.2872340425531915  │ 0.3253968253968254  │\n",
      "├───────────┼─────────────────────┼─────────────────────┤\n",
      "│ DEPRES(1) │ 0.22340425531914893 │ 0.19047619047619047 │\n",
      "├───────────┼─────────────────────┼─────────────────────┤\n",
      "│ DEPRES(2) │ 0.30851063829787234 │ 0.1984126984126984  │\n",
      "├───────────┼─────────────────────┼─────────────────────┤\n",
      "│ DEPRES(4) │ 0.1276595744680851  │ 0.19047619047619047 │\n",
      "├───────────┼─────────────────────┼─────────────────────┤\n",
      "│ DEPRES(5) │ 0.05319148936170213 │ 0.09523809523809523 │\n",
      "╘═══════════╧═════════════════════╧═════════════════════╛\n",
      "\n",
      " ╒══════════╤═════════════════════╤════════════════════╕\n",
      "│ OPI      │ OPI(0.0)            │ OPI(1.0)           │\n",
      "├──────────┼─────────────────────┼────────────────────┤\n",
      "│ SEX(0.0) │ 0.5531914893617021  │ 0.4444444444444444 │\n",
      "├──────────┼─────────────────────┼────────────────────┤\n",
      "│ SEX(1.0) │ 0.44680851063829785 │ 0.5555555555555556 │\n",
      "╘══════════╧═════════════════════╧════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import BayesianEstimator\n",
    "est = BayesianEstimator(model, data)\n",
    "\n",
    "equivalent_sample_size = 20\n",
    "print(\"\\n\", est.estimate_cpd('OPI', prior_type='BDeu', equivalent_sample_size=equivalent_sample_size))  # unconditional\n",
    "print(\"\\n\", est.estimate_cpd('FREQ', prior_type='BDeu', equivalent_sample_size=equivalent_sample_size))  # conditional on OPI\n",
    "print(\"\\n\", est.estimate_cpd('NRS', prior_type='BDeu', equivalent_sample_size=equivalent_sample_size))  # conditional on OPI\n",
    "print(\"\\n\", est.estimate_cpd('RAD', prior_type='BDeu', equivalent_sample_size=equivalent_sample_size))  # conditional on OPI\n",
    "print(\"\\n\", est.estimate_cpd('JOI', prior_type='BDeu', equivalent_sample_size=equivalent_sample_size))  # conditional on OPI\n",
    "print(\"\\n\", est.estimate_cpd('LBS', prior_type='BDeu', equivalent_sample_size=equivalent_sample_size))  # conditional on OPI\n",
    "print(\"\\n\", est.estimate_cpd('PICHOR', prior_type='BDeu', equivalent_sample_size=equivalent_sample_size))  # conditional on OPI\n",
    "print(\"\\n\", est.estimate_cpd('INJ', prior_type='BDeu', equivalent_sample_size=equivalent_sample_size))  # conditional on OPI\n",
    "print(\"\\n\", est.estimate_cpd('EXE', prior_type='BDeu', equivalent_sample_size=equivalent_sample_size))  # conditional on OPI\n",
    "print(\"\\n\", est.estimate_cpd('DIS', prior_type='BDeu', equivalent_sample_size=equivalent_sample_size))  # conditional on OPI\n",
    "print(\"\\n\", est.estimate_cpd('DEPRES', prior_type='BDeu', equivalent_sample_size=equivalent_sample_size))  # conditional on OPI\n",
    "print(\"\\n\", est.estimate_cpd('SEX', prior_type='BDeu', equivalent_sample_size=equivalent_sample_size))  # conditional on OPI\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The estimated values in the CPDs are now more conservative. In particular, the estimate for  OPI 0/1 was 0.42/0.58 with a sample size = 20 is smoothed to 0.43/0.57. Large values for equivalent sample size or reducing the N samples would result in increasing more conservative estiamtes.\n",
    "\n",
    "`BayesianEstimator`, too, can be used via the `fit()`-method. Full example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Replacing existing CPD for SEX\n",
      "WARNING:root:Replacing existing CPD for DEPRES\n",
      "WARNING:root:Replacing existing CPD for PICHOR\n",
      "WARNING:root:Replacing existing CPD for DIS\n",
      "WARNING:root:Replacing existing CPD for INJ\n",
      "WARNING:root:Replacing existing CPD for DUR\n",
      "WARNING:root:Replacing existing CPD for OPI\n",
      "WARNING:root:Replacing existing CPD for FREQ\n",
      "WARNING:root:Replacing existing CPD for RAD\n",
      "WARNING:root:Replacing existing CPD for JOI\n",
      "WARNING:root:Replacing existing CPD for LBS\n",
      "WARNING:root:Replacing existing CPD for EXE\n",
      "WARNING:root:Replacing existing CPD for NRS\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "╒═══════════╤═════════════════════╤═════════════════════╕\n",
      "│ OPI       │ OPI(0.0)            │ OPI(1.0)            │\n",
      "├───────────┼─────────────────────┼─────────────────────┤\n",
      "│ DEPRES(0) │ 0.2947976878612717  │ 0.3333333333333333  │\n",
      "├───────────┼─────────────────────┼─────────────────────┤\n",
      "│ DEPRES(1) │ 0.2254335260115607  │ 0.189873417721519   │\n",
      "├───────────┼─────────────────────┼─────────────────────┤\n",
      "│ DEPRES(2) │ 0.3179190751445087  │ 0.19831223628691982 │\n",
      "├───────────┼─────────────────────┼─────────────────────┤\n",
      "│ DEPRES(4) │ 0.12138728323699421 │ 0.189873417721519   │\n",
      "├───────────┼─────────────────────┼─────────────────────┤\n",
      "│ DEPRES(5) │ 0.04046242774566474 │ 0.08860759493670886 │\n",
      "╘═══════════╧═════════════════════╧═════════════════════╛\n",
      "╒══════════╤═════════════════════╤════════════════════╕\n",
      "│ OPI      │ OPI(0.0)            │ OPI(1.0)           │\n",
      "├──────────┼─────────────────────┼────────────────────┤\n",
      "│ DIS(0.0) │ 0.9624277456647399  │ 0.7531645569620253 │\n",
      "├──────────┼─────────────────────┼────────────────────┤\n",
      "│ DIS(1.0) │ 0.03757225433526012 │ 0.2468354430379747 │\n",
      "╘══════════╧═════════════════════╧════════════════════╛\n",
      "╒════════╤══════════════════════╤══════════════════════╕\n",
      "│ OPI    │ OPI(0.0)             │ OPI(1.0)             │\n",
      "├────────┼──────────────────────┼──────────────────────┤\n",
      "│ DUR(1) │ 0.04190751445086705  │ 0.022151898734177215 │\n",
      "├────────┼──────────────────────┼──────────────────────┤\n",
      "│ DUR(2) │ 0.030346820809248554 │ 0.04746835443037975  │\n",
      "├────────┼──────────────────────┼──────────────────────┤\n",
      "│ DUR(3) │ 0.3540462427745665   │ 0.30063291139240506  │\n",
      "├────────┼──────────────────────┼──────────────────────┤\n",
      "│ DUR(4) │ 0.5736994219653179   │ 0.629746835443038    │\n",
      "╘════════╧══════════════════════╧══════════════════════╛\n",
      "╒══════════╤════════════════════╤════════════════════╕\n",
      "│ OPI      │ OPI(0.0)           │ OPI(1.0)           │\n",
      "├──────────┼────────────────────┼────────────────────┤\n",
      "│ EXE(0.0) │ 0.3265895953757225 │ 0.0949367088607595 │\n",
      "├──────────┼────────────────────┼────────────────────┤\n",
      "│ EXE(1.0) │ 0.6734104046242775 │ 0.9050632911392406 │\n",
      "╘══════════╧════════════════════╧════════════════════╛\n",
      "╒═════════╤═════════════════════╤═════════════════════╕\n",
      "│ OPI     │ OPI(0.0)            │ OPI(1.0)            │\n",
      "├─────────┼─────────────────────┼─────────────────────┤\n",
      "│ FREQ(1) │ 0.18304431599229287 │ 0.10829817158931083 │\n",
      "├─────────┼─────────────────────┼─────────────────────┤\n",
      "│ FREQ(2) │ 0.2639691714836223  │ 0.2939521800281294  │\n",
      "├─────────┼─────────────────────┼─────────────────────┤\n",
      "│ FREQ(3) │ 0.5529865125240848  │ 0.5977496483825597  │\n",
      "╘═════════╧═════════════════════╧═════════════════════╛\n",
      "╒══════════╤═════════════════════╤════════════════════╕\n",
      "│ OPI      │ OPI(0.0)            │ OPI(1.0)           │\n",
      "├──────────┼─────────────────────┼────────────────────┤\n",
      "│ INJ(0.0) │ 0.8352601156069365  │ 0.4831223628691983 │\n",
      "├──────────┼─────────────────────┼────────────────────┤\n",
      "│ INJ(1.0) │ 0.16473988439306358 │ 0.5168776371308017 │\n",
      "╘══════════╧═════════════════════╧════════════════════╛\n",
      "╒════════╤═════════════════════╤════════════════════╕\n",
      "│ OPI    │ OPI(0.0)            │ OPI(1.0)           │\n",
      "├────────┼─────────────────────┼────────────────────┤\n",
      "│ JOI(0) │ 0.2639691714836223  │ 0.1842475386779184 │\n",
      "├────────┼─────────────────────┼────────────────────┤\n",
      "│ JOI(1) │ 0.46050096339113683 │ 0.3867791842475387 │\n",
      "├────────┼─────────────────────┼────────────────────┤\n",
      "│ JOI(2) │ 0.27552986512524086 │ 0.4289732770745429 │\n",
      "╘════════╧═════════════════════╧════════════════════╛\n",
      "╒════════╤══════════════════════╤═════════════════════╕\n",
      "│ OPI    │ OPI(0.0)             │ OPI(1.0)            │\n",
      "├────────┼──────────────────────┼─────────────────────┤\n",
      "│ LBS(0) │ 0.9344894026974953   │ 0.7834036568213784  │\n",
      "├────────┼──────────────────────┼─────────────────────┤\n",
      "│ LBS(1) │ 0.021194605009633917 │ 0.13361462728551338 │\n",
      "├────────┼──────────────────────┼─────────────────────┤\n",
      "│ LBS(2) │ 0.044315992292870914 │ 0.08298171589310831 │\n",
      "╘════════╧══════════════════════╧═════════════════════╛\n",
      "╒═════════╤═══════════════════════╤═══════════════════════╕\n",
      "│ OPI     │ OPI(0.0)              │ OPI(1.0)              │\n",
      "├─────────┼───────────────────────┼───────────────────────┤\n",
      "│ NRS(0)  │ 0.014188124014713609  │ 0.0019179133103183737 │\n",
      "├─────────┼───────────────────────┼───────────────────────┤\n",
      "│ NRS(1)  │ 0.0026274303730951126 │ 0.027234369006520907  │\n",
      "├─────────┼───────────────────────┼───────────────────────┤\n",
      "│ NRS(2)  │ 0.09511297950604307   │ 0.052550824702723445  │\n",
      "├─────────┼───────────────────────┼───────────────────────┤\n",
      "│ NRS(3)  │ 0.14135575407251705   │ 0.11162255466052935   │\n",
      "├─────────┼───────────────────────┼───────────────────────┤\n",
      "│ NRS(4)  │ 0.16447714135575403   │ 0.11162255466052935   │\n",
      "├─────────┼───────────────────────┼───────────────────────┤\n",
      "│ NRS(5)  │ 0.12979506043089856   │ 0.12850019179133104   │\n",
      "├─────────┼───────────────────────┼───────────────────────┤\n",
      "│ NRS(6)  │ 0.12979506043089856   │ 0.1791331031837361    │\n",
      "├─────────┼───────────────────────┼───────────────────────┤\n",
      "│ NRS(7)  │ 0.22228060956384652   │ 0.17069428461833527   │\n",
      "├─────────┼───────────────────────┼───────────────────────┤\n",
      "│ NRS(8)  │ 0.07199159222280609   │ 0.1031837360951285    │\n",
      "├─────────┼───────────────────────┼───────────────────────┤\n",
      "│ NRS(9)  │ 0.014188124014713609  │ 0.07786728039892597   │\n",
      "├─────────┼───────────────────────┼───────────────────────┤\n",
      "│ NRS(10) │ 0.014188124014713609  │ 0.035673187571921755  │\n",
      "╘═════════╧═══════════════════════╧═══════════════════════╛\n",
      "╒══════════╤══════════╕\n",
      "│ OPI(0.0) │ 0.421951 │\n",
      "├──────────┼──────────┤\n",
      "│ OPI(1.0) │ 0.578049 │\n",
      "╘══════════╧══════════╛\n",
      "╒═══════════╤══════════════════════╤══════════════════════╕\n",
      "│ OPI       │ OPI(0.0)             │ OPI(1.0)             │\n",
      "├───────────┼──────────────────────┼──────────────────────┤\n",
      "│ PICHOR(0) │ 0.004816955684007707 │ 0.011954992967651195 │\n",
      "├───────────┼──────────────────────┼──────────────────────┤\n",
      "│ PICHOR(1) │ 0.07418111753371869  │ 0.03727144866385373  │\n",
      "├───────────┼──────────────────────┼──────────────────────┤\n",
      "│ PICHOR(2) │ 0.22447013487475914  │ 0.18073136427566805  │\n",
      "├───────────┼──────────────────────┼──────────────────────┤\n",
      "│ PICHOR(3) │ 0.2822736030828516   │ 0.1891701828410689   │\n",
      "├───────────┼──────────────────────┼──────────────────────┤\n",
      "│ PICHOR(4) │ 0.3053949903660886   │ 0.3326300984528832   │\n",
      "├───────────┼──────────────────────┼──────────────────────┤\n",
      "│ PICHOR(5) │ 0.10886319845857416  │ 0.2482419127988748   │\n",
      "╘═══════════╧══════════════════════╧══════════════════════╛\n",
      "╒══════════╤════════════════════╤═════════════════════╕\n",
      "│ OPI      │ OPI(0.0)           │ OPI(1.0)            │\n",
      "├──────────┼────────────────────┼─────────────────────┤\n",
      "│ RAD(0.0) │ 0.6734104046242775 │ 0.5084388185654009  │\n",
      "├──────────┼────────────────────┼─────────────────────┤\n",
      "│ RAD(1.0) │ 0.3265895953757225 │ 0.49156118143459915 │\n",
      "╘══════════╧════════════════════╧═════════════════════╛\n",
      "╒══════════╤════════════════════╤════════════════════╕\n",
      "│ OPI      │ OPI(0.0)           │ OPI(1.0)           │\n",
      "├──────────┼────────────────────┼────────────────────┤\n",
      "│ SEX(0.0) │ 0.5578034682080925 │ 0.4409282700421941 │\n",
      "├──────────┼────────────────────┼────────────────────┤\n",
      "│ SEX(1.0) │ 0.4421965317919075 │ 0.5590717299578059 │\n",
      "╘══════════╧════════════════════╧════════════════════╛\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.estimators import BayesianEstimator\n",
    "\n",
    "model.fit(data, estimator=BayesianEstimator, prior_type=\"BDeu\") \n",
    "# default equivalent_sample_size=5\n",
    "for cpd in model.get_cpds():\n",
    "    print(cpd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Structure Learning\n",
    "\n",
    "To learn model structure (a DAG) from a data set, there are two broad techniques:\n",
    "\n",
    " - score-based structure learning\n",
    " - constraint-based structure learning\n",
    "\n",
    "The combination of both techniques allows further improvement:\n",
    " - hybrid structure learning\n",
    "\n",
    "We briefly discuss all approaches and give examples.\n",
    "\n",
    "### Score-based Structure Learning\n",
    "\n",
    "\n",
    "This approach construes model selection as an optimization task. It has two building blocks:\n",
    "\n",
    "- A _scoring function_ $s_D\\colon M \\to \\mathbb R$ that maps models to a numerical score, based on how well they fit to a given data set $D$.\n",
    "- A _search strategy_ to traverse the search space of possible models $M$ and select a model with optimal score.\n",
    "\n",
    "\n",
    "#### Scoring functions\n",
    "\n",
    "Commonly used scores to measure the fit between model and data are _Bayesian Dirichlet scores_ such as *BDeu* or *K2* and the _Bayesian Information Criterion_ (BIC, also called MDL). See [1], Section 18.3 for a detailed introduction on scores. As before, BDeu is dependent on an equivalent sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-13934.716633404805\n",
      "-14325.39375557494\n",
      "-14290.759744\n",
      "-20893.440702569555\n",
      "-20920.27712071741\n",
      "-20937.4995007\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pgmpy.estimators import BdeuScore, K2Score, BicScore\n",
    "from pgmpy.models import BayesianModel\n",
    "\n",
    "# create random data sample with 3 variables, where Z is dependent on X, Y:\n",
    "data0 = pd.DataFrame(np.random.randint(0, 4, size=(5000, 2)), columns=list('XY'))\n",
    "data0['Z'] = data0['X'] + data0['Y']\n",
    "\n",
    "bdeu = BdeuScore(data0, equivalent_sample_size=5)\n",
    "k2 = K2Score(data0)\n",
    "bic = BicScore(data0)\n",
    "\n",
    "model1 = BayesianModel([('X', 'Z'), ('Y', 'Z')])  # X -> Z <- Y\n",
    "model2 = BayesianModel([('X', 'Z'), ('X', 'Y')])  # Y <- X -> Z\n",
    "\n",
    "\n",
    "print(bdeu.score(model1))\n",
    "print(k2.score(model1))\n",
    "print(bic.score(model1))\n",
    "\n",
    "print(bdeu.score(model2))\n",
    "print(k2.score(model2))\n",
    "print(bic.score(model2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the scores vary slightly, we can see that the correct `model1` has a much higher score than `model2`.\n",
    "Importantly, these scores _decompose_, i.e. they can be computed locally for each of the variables given their potential parents, independent of other parts of the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9217.130247825968\n",
      "-6989.664089188232\n",
      "-57.11390342056143\n"
     ]
    }
   ],
   "source": [
    "print(bdeu.local_score('Z', parents=[]))\n",
    "print(bdeu.local_score('Z', parents=['X']))\n",
    "print(bdeu.local_score('Z', parents=['X', 'Y']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search strategies\n",
    "The search space of DAGs is super-exponential in the number of variables and the above scoring functions allow for local maxima. The first property makes exhaustive search intractable for all but very small networks, the second prohibits efficient local optimization algorithms to always find the optimal structure. Thus, identifiying the ideal structure is often not tractable. Despite these bad news, heuristic search strategies often yields good results.\n",
    "\n",
    "If only few nodes are involved (read: less than 5), `ExhaustiveSearch` can be used to compute the score for every DAG and returns the best-scoring one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('INJ', 'LBS'), ('INJ', 'OPI')]\n",
      "\n",
      "All DAGs by score:\n",
      "-349.556280976 [('LBS', 'INJ'), ('INJ', 'OPI')]\n",
      "-349.556280976 [('OPI', 'INJ'), ('INJ', 'LBS')]\n",
      "-349.556280976 [('INJ', 'LBS'), ('INJ', 'OPI')]\n",
      "-356.948794298 [('OPI', 'LBS'), ('OPI', 'INJ'), ('INJ', 'LBS')]\n",
      "-356.948794298 [('LBS', 'INJ'), ('OPI', 'LBS'), ('OPI', 'INJ')]\n",
      "-356.948794298 [('LBS', 'OPI'), ('LBS', 'INJ'), ('OPI', 'INJ')]\n",
      "-356.948794298 [('LBS', 'OPI'), ('LBS', 'INJ'), ('INJ', 'OPI')]\n",
      "-356.948794298 [('OPI', 'LBS'), ('INJ', 'LBS'), ('INJ', 'OPI')]\n",
      "-356.948794298 [('LBS', 'OPI'), ('INJ', 'LBS'), ('INJ', 'OPI')]\n",
      "-358.417818546 [('LBS', 'INJ'), ('OPI', 'INJ')]\n",
      "-360.156180231 [('LBS', 'INJ'), ('OPI', 'LBS')]\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import ExhaustiveSearch\n",
    "\n",
    "# reduce number of random variables\n",
    "from pgmpy.estimators import ExhaustiveSearch\n",
    "\n",
    "#data2 = data[['OPI', 'INJ', 'LBS','PICHOR', 'NRS', 'RAD', 'JOI']]\n",
    "\n",
    "data2 = data[['INJ', 'LBS','OPI']]\n",
    "\n",
    "bdeu = BdeuScore(data2, equivalent_sample_size=5)\n",
    "k2 = K2Score(data2)\n",
    "bic = BicScore(data2)\n",
    "\n",
    "es = ExhaustiveSearch(data2, scoring_method=bic)\n",
    "best_model = es.estimate()\n",
    "print(best_model.edges())\n",
    "\n",
    "print(\"\\nAll DAGs by score:\")\n",
    "count = 0\n",
    "for score, dag in reversed(es.all_scores()):\n",
    "    print(score, dag.edges())\n",
    "    count += 1\n",
    "    if count>10:\n",
    "        break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once more nodes are involved, one needs to switch to heuristic search. `HillClimbSearch` implements a greedy local search that starts from the DAG `start` (default: disconnected DAG) and proceeds by iteratively performing single-edge manipulations that maximally increase the score. The search terminates once a local maximum is found.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('PICHOR', 'RAD'), ('INJ', 'LBS'), ('INJ', 'DIS'), ('INJ', 'FREQ'), ('OPI', 'DIS'), ('OPI', 'INJ'), ('OPI', 'EXE'), ('RAD', 'JOI'), ('RAD', 'OPI')]\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import HillClimbSearch\n",
    "\n",
    "hc = HillClimbSearch(data, scoring_method=BicScore(data))\n",
    "best_model = hc.estimate()\n",
    "print(best_model.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To enforce a wider exploration of the search space, the search can be enhanced with a tabu list. The list keeps track of the last `n` modfications; those are then not allowed to be reversed, regardless of the score. Additionally a `white_list` or `black_list` can be supplied to restrict the search to a particular subset or to exclude certain edges. The parameter `max_indegree` allows to restrict the maximum number of parents for each node.\n",
    "\n",
    "\n",
    "### Constraint-based Structure Learning\n",
    "\n",
    "A different, but quite straightforward approach to build a DAG from data is this:\n",
    "\n",
    "1. Identify independencies in the data set using hypothesis tests \n",
    "2. Construct DAG (pattern) according to identified independencies\n",
    "\n",
    "#### (Conditional) Independence Tests\n",
    "\n",
    "Independencies in the data can be identified using chi2 conditional independence tests. To this end, constraint-based estimators in pgmpy have a `test_conditional_independence(X, Y, Zs)`-method, that performs a hypothesis test on the data sample. It allows to check if `X` is independent from `Y` given a set of variables `Zs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(662.12032349956701, 6.2651022777027284e-123, True)\n",
      "(4.6402848339816627, 0.94732925952156943, True)\n",
      "(15.872232810066384, 0.99997026665122879, True)\n",
      "(5.7183082660601627, 0.99922794955246963, True)\n",
      "(4618.0, 0.0, True)\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import ConstraintBasedEstimator\n",
    "\n",
    "# sample data\n",
    "data00 = pd.DataFrame(np.random.randint(0, 3, size=(2500, 8)), columns=list('ABCDEFGH'))\n",
    "data00['A'] += data00['B'] + data00['C']\n",
    "data00['H'] = data00['G'] - data00['A']\n",
    "data00['E'] *= data00['F']\n",
    "\n",
    "est = ConstraintBasedEstimator(data00)\n",
    "\n",
    "print(est.test_conditional_independence('B', 'H'))          # dependent\n",
    "print(est.test_conditional_independence('B', 'E'))          # independent\n",
    "print(est.test_conditional_independence('B', 'H', ['A']))   # independent\n",
    "print(est.test_conditional_independence('A', 'G'))          # independent\n",
    "print(est.test_conditional_independence('A', 'G',  ['H']))  # dependent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27.618973230613527, 4.3662414579369054e-06, True)\n",
      "(11.255346164127243, 0.046545787789089264, True)\n",
      "(40.180558029163208, 1.3732531235567979e-07, True)\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.estimators import ConstraintBasedEstimator\n",
    "\n",
    "est = ConstraintBasedEstimator(data)\n",
    "\n",
    "# ['DUR', 'FREQ', 'NRS', 'RAD', 'JOI', 'LBS', 'PICHOR', 'OPI', 'INJ', 'EXE', 'DIS', 'DEPRES', 'SEX']\n",
    "\n",
    "print(est.test_conditional_independence('OPI', 'INJ'))         \n",
    "print(est.test_conditional_independence('OPI', 'LBS'))          \n",
    "print(est.test_conditional_independence('LBS', 'INJ'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`test_conditional_independence()` returns a tripel `(chi2, p_value, sufficient_data)`, consisting in the computed chi2 test statistic, the `p_value` of the test, and a heuristig flag that indicates if the sample size was sufficient. The `p_value` is the probability of observing the computed chi2 statistic (or an even higher chi2 value), given the null hypothesis that X and Y are independent given Zs.\n",
    "\n",
    "This can be used to make independence judgements, at a given level of significance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def is_independent(X, Y, Zs=[], significance_level=0.05):\n",
    "    return est.test_conditional_independence(X, Y, Zs)[1] >= significance_level\n",
    "\n",
    "print(is_independent('OPI', 'INJ'))\n",
    "print(is_independent('OPI', 'LBS'))\n",
    "print(is_independent('LBS', 'INJ'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DAG (pattern) construction\n",
    "\n",
    "With a method for independence testing at hand, we can construct a DAG from the data set in three steps:\n",
    "1. Construct an undirected skeleton - `estimate_skeleton()`\n",
    "2. Orient compelled edges to obtain partially directed acyclid graph (PDAG; I-equivalence class of DAGs) - `skeleton_to_pdag()`\n",
    "3. Extend DAG pattern to a DAG by conservatively orienting the remaining edges in some way - `pdag_to_dag()`\n",
    "\n",
    "Step 1.&2. form the so-called PC algorithm, see [2], page 550. PDAGs are `DirectedGraph`s, that may contain both-way edges, to indicate that the orientation for the edge is not determined.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Undirected edges:  [('PICHOR', 'NRS'), ('INJ', 'OPI'), ('INJ', 'LBS'), ('JOI', 'RAD')]\n",
      "PDAG edges:        [('PICHOR', 'NRS'), ('OPI', 'INJ'), ('JOI', 'RAD'), ('RAD', 'JOI'), ('LBS', 'INJ'), ('NRS', 'PICHOR')]\n",
      "DAG edges:         [('OPI', 'INJ'), ('RAD', 'JOI'), ('LBS', 'INJ'), ('NRS', 'PICHOR')]\n"
     ]
    }
   ],
   "source": [
    "skel, seperating_sets = est.estimate_skeleton(significance_level=0.01)\n",
    "print(\"Undirected edges: \", skel.edges())\n",
    "\n",
    "pdag = est.skeleton_to_pdag(skel, seperating_sets)\n",
    "print(\"PDAG edges:       \", pdag.edges())\n",
    "\n",
    "model = est.pdag_to_dag(pdag)\n",
    "print(\"DAG edges:        \", model.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `estimate()`-method provides a shorthand for the three steps above and directly returns a `BayesianModel`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('OPI', 'INJ'), ('RAD', 'JOI'), ('LBS', 'INJ'), ('NRS', 'PICHOR')]\n"
     ]
    }
   ],
   "source": [
    "print(est.estimate(significance_level=0.01).edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `estimate_from_independencies()`-method can be used to construct a `BayesianModel` from a provided *set of independencies* (see class documentation for further features & methods):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LBS', 'INJ'), ('LBS', 'PICHOR'), ('LBS', 'NRS'), ('OPI', 'LBS'), ('OPI', 'JOI'), ('OPI', 'INJ'), ('OPI', 'PICHOR'), ('OPI', 'NRS'), ('PICHOR', 'NRS'), ('RAD', 'JOI'), ('RAD', 'OPI'), ('RAD', 'PICHOR'), ('RAD', 'NRS'), ('JOI', 'PICHOR'), ('JOI', 'NRS'), ('INJ', 'PICHOR'), ('INJ', 'NRS')]\n"
     ]
    }
   ],
   "source": [
    "from pgmpy.independencies import Independencies\n",
    "\n",
    "ind = Independencies([('LBS', 'INJ'), ('JOI', 'RAD'), ('OPI', 'INJ'), ('PICHOR', 'NRS')])\n",
    "ind = ind.closure()  # required (!) for faithfulness\n",
    "\n",
    "model = ConstraintBasedEstimator.estimate_from_independencies(['LBS', 'INJ', 'JOI', 'RAD', 'OPI', 'INJ', 'PICHOR', 'NRS'], ind)\n",
    "\n",
    "print(model.edges())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model0 Naive Bayes\n",
      "-2503.840315758571\n",
      "-2491.6320735221907\n",
      "-2541.02633182\n",
      "model1 Hill Climbing\n",
      "-1361.3205978300227\n",
      "-1362.1412456048588\n",
      "-1377.64616565\n",
      "model2 Constraint Learning\n",
      "-1474.6274320755251\n",
      "-1445.220201410989\n",
      "-1550.31881365\n",
      "model3 Independencies from Constraint Learned Model\n",
      "-1966.341356309149\n",
      "-1507.7318695562417\n",
      "-13530.0982644\n",
      "model4 Independencies from Constraint Learned Model\n",
      "-1269.666550422272\n",
      "-1268.2079060428323\n",
      "-1285.53468756\n"
     ]
    }
   ],
   "source": [
    "bdeu = BdeuScore(data, equivalent_sample_size=5)\n",
    "k2 = K2Score(data)\n",
    "bic = BicScore(data)\n",
    "\n",
    "# naive bayes model\n",
    "model0 = BayesianModel([('OPI','DUR'), ('OPI','FREQ'), ('OPI','NRS'), ('OPI','RAD'), ('OPI','JOI'), ('OPI','LBS'), ('OPI','PICHOR'), ('OPI','INJ'), ('OPI','EXE'), ('OPI','DIS'), ('OPI','DEPRES'), ('OPI','SEX')])\n",
    "# hill climbing model\n",
    "model1 = BayesianModel([('RAD', 'PICHOR'), ('RAD', 'JOI'), ('RAD', 'OPI'), ('OPI', 'DIS'), ('OPI', 'INJ'), ('OPI', 'EXE'), ('INJ', 'DIS'), ('INJ', 'LBS'), ('INJ', 'FREQ')])\n",
    "# constraint model learned from pair-wise independencies\n",
    "model2 = BayesianModel([('LBS', 'INJ'), ('JOI', 'RAD'), ('OPI', 'INJ'), ('PICHOR', 'NRS')])  \n",
    "# model built using indepencies estimated from model 2\n",
    "model3 = BayesianModel([('PICHOR', 'OPI'), ('PICHOR', 'NRS'), ('INJ', 'PICHOR'), ('INJ', 'OPI'), ('INJ', 'NRS'), ('RAD', 'PICHOR'), ('RAD', 'JOI'), ('RAD', 'OPI'), ('RAD', 'NRS'), ('LBS', 'PICHOR'), ('LBS', 'INJ'), ('LBS', 'OPI'), ('LBS', 'NRS'), ('OPI', 'NRS'), ('JOI', 'PICHOR'), ('JOI', 'OPI'), ('JOI', 'NRS')])  \n",
    "# domain expert model\n",
    "model4 = BayesianModel([('DIS', 'OPI'), ('DIS', 'INJ'), ('OPI', 'RAD'), ('OPI', 'INJ'), ('RAD', 'PICHOR'), ('RAD', 'JOI'), ('INJ', 'FREQ'), ('INJ', 'LBS')])\n",
    "\n",
    "print(\"model0 Naive Bayes\")\n",
    "print(bdeu.score(model0))\n",
    "print(k2.score(model0))\n",
    "print(bic.score(model0))\n",
    "\n",
    "print(\"model1 Hill Climbing\")\n",
    "print(bdeu.score(model1))\n",
    "print(k2.score(model1))\n",
    "print(bic.score(model1))\n",
    "\n",
    "print(\"model2 Constraint Learning\")\n",
    "print(bdeu.score(model2))\n",
    "print(k2.score(model2))\n",
    "print(bic.score(model2))\n",
    "\n",
    "print(\"model3 Independencies from Constraint Learned Model\")\n",
    "print(bdeu.score(model3))\n",
    "print(k2.score(model3))\n",
    "print(bic.score(model3))\n",
    "\n",
    "print(\"model4 Medical domain expert defined model\")\n",
    "print(bdeu.score(model4))\n",
    "print(k2.score(model4))\n",
    "print(bic.score(model4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model 4, defined by Dr. Meredith Adams provided the best fit to the data, followed by Hill Climbing using Bayesian Information criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PC PDAG construction is only guaranteed to work under the assumption that the identified set of independencies is *faithful*, i.e. there exists a DAG that exactly corresponds to it. Spurious dependencies in the data set can cause the reported independencies to violate faithfulness. It can happen that the estimated PDAG does not have any faithful completions (i.e. edge orientations that do not introduce new v-structures). In that case a warning is issued.\n",
    "\n",
    "\n",
    "### Hybrid Structure Learning\n",
    "\n",
    "The MMHC algorithm [3] combines the constraint-based and score-based method. It has two parts:\n",
    "\n",
    "1. Learn undirected graph skeleton using the constraint-based construction procedure MMPC\n",
    "2. Orient edges using score-based optimization (BDeu score + modified hill-climbing)\n",
    "\n",
    "This method is not functional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MmhcEstimator.estimate()` is a shorthand for both steps and directly estimates a `BayesianModel`.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "This notebook aimed to give an overview of pgmpy's estimators for learning Bayesian network structure and parameters. For more information about the individual functions see their docstring documentation. If you used pgmpy's structure learning features to satisfactorily learn a non-trivial network from real data, feel free to drop us an eMail via the mailing list or just open a Github issue. We'd like to put your network in the examples-section!\n",
    "\n",
    "### References\n",
    "\n",
    "[1] Koller & Friedman, Probabilistic Graphical Models - Principles and Techniques, 2009\n",
    "\n",
    "[2] Neapolitan, [Learning Bayesian Networks](http://www.cs.technion.ac.il/~dang/books/Learning%20Bayesian%20Networks&#40;Neapolitan,%20Richard&#41;.pdf), 2003\n",
    "\n",
    "[3] Tsamardinos et al., [The max-min hill-climbing BN structure learning algorithm](http://www.dsl-lab.org/supplements/mmhc_paper/paper_online.pdf), 2005\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pgmpy35x1",
   "language": "python",
   "name": "pgmpy35x1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
